\section{Measurement Setup}
\label{sec:measurement_setup}

To research the influence of different initial window size on TCP and the
network, we chose the network virtualization framework Mininet~\cite{mininet}.
Mininet exploits existing os virtualization and resource management features of
the Linux kernel, namely Network namespaces~\cite{network_namespaces} and
Cgroups~\cite{cgroups}, to simulate multiple networks and peers on a single
host. Because no virtual machines are involved and Linux can make advantage of
zero-copy mechanism, the overhead of these Network namespaces is low. It can
easily simulate 10GbE-Networks using commodity PC-Hardware. Mininet also
integrates other features such as traffic control and OpenFlow, so one can build
arbitrary network topologies and conditions. To create and configure networks
Mininet exposes a Python API and allow to interact with network namespaces at
runtime by giving shell access.

\input{topology}

For our network topology we have chosen a setup as depicted in figure~\ref{fig:topology}.
Both client and server are represented by host. They are connected via virtual
ethnernet pairs to a software bridge, which represents our router. The veth pair
of the server stands for the Uplink the client has over the internet to our
server. The other one is the local area network, where both client and router
are included. To limit the bandwith and set a forwarding delay, we applied
policies using Linux's Traffic Control~\cite{tc}. The Link between client and
router was constantly limited to 1GB with a forwarding delay of 1ms. The TCP
receive/send window was globally with to be large enough to fit even the largest
requests made during this experiment. The relevant sysctl configuration is the
following:

\begin{lstlisting}
# /etc/sysctl.conf
net.ipv4.tcp_window_scaling = 1

net.core.wmem_max = 16777216
net.ipv4.tcp_wmem = 10240 87380 16777216
net.ipv4.tcp_rmem = 10240 87380 16777216
\end{lstlisting}

As scheduling algorithm we used HFSC. As TCP congestion algorithm
Cubic~\cite{cubic} was in use, which is the default on Linux. As application of
TCP we decied to use HTTP. Therefor we started nginx in version 1.8.0 on the the
server and set it up to serve static files:

\begin{lstlisting}
# nginx.conf
pid nginx.pid;
user root;
error_log stderr;
events { worker_connections  1024; }
http { server {
    listen 80;
    location / { root www; }
} }
\end{lstlisting}

On the client, we use cURL~\cite{curl} to issue HTTP Requests. To match up
better with real browsers in term of request size the HTTP header of Google
Chrome was used.

%# Testsetup
%
%- Dateien von 1kb bis 16384kb
%- neu starten, nach jeder anfrage
%
%## Grenzen der Simulation:
%- Nur ein Link zum Server simuliert
%  -> aber häufig der Flaschhals zwischen Client und Server
%- Nur den Linux-TCP Stack und dessen Standardeinstellungen betrachtet
%- Nur symmetrische Bandbreiten getestet
%  -> sollte keinen Einfluss haben,
%  da Acknowledges verhältnissmäßig zur übertragenen Nachricht klein sind.
%
%- bw = (cwnd * mtu) / rtt
%- rto... retransmission timeout
%  - time before unacknowledged packet is resent
%  - zu groß -> Verzögerungen, doppelte acks.
%  - zu klein -> doppelte Packete verschicken
%  - z.B. laut RFC rto = srtt + 4 * varrtt
